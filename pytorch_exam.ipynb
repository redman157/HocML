{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/redman157/HocML/blob/master/pytorch_exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r9aUcVECieF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "7803d45c-1ed2-4c45-cdca-f199d2cc31fa"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "def imshow(img):\n",
        "  img = img / 2\n",
        "  npimg = img.numpy()\n",
        "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "  plt.show()\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 170041344/170498071 [00:15<00:00, 7991010.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "plane   cat   car  frog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3-2CQuRFdB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "net = Net()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnNBuECTGQMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "2f86a7b0-07eb-49be-f0b8-6b3904ce854a"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 2.217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r170500096it [00:30, 7991010.31it/s]                               "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,  4000] loss: 1.868\n",
            "[1,  6000] loss: 1.675\n",
            "[1,  8000] loss: 1.557\n",
            "[1, 10000] loss: 1.504\n",
            "[1, 12000] loss: 1.436\n",
            "[2,  2000] loss: 1.390\n",
            "[2,  4000] loss: 1.360\n",
            "[2,  6000] loss: 1.333\n",
            "[2,  8000] loss: 1.305\n",
            "[2, 10000] loss: 1.287\n",
            "[2, 12000] loss: 1.277\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjU4pNkfG9yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "\n",
        "outputs = net(images)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "imshow(torchvision.utils.make_grid(_))\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
        "                              for j in range(4)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0xvv1WbGjVW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2f7d8a36-d5b3-41fe-efef-17c38811df26"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 55 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOGHEZ9JRlVZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "1d892290-243a-464e-d813-4f1ec0cf3882"
      },
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "          x`\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of plane : 51 %\n",
            "Accuracy of   car : 70 %\n",
            "Accuracy of  bird : 34 %\n",
            "Accuracy of   cat : 48 %\n",
            "Accuracy of  deer : 37 %\n",
            "Accuracy of   dog : 40 %\n",
            "Accuracy of  frog : 71 %\n",
            "Accuracy of horse : 66 %\n",
            "Accuracy of  ship : 71 %\n",
            "Accuracy of truck : 66 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpaTqpIXaYo8",
        "colab_type": "text"
      },
      "source": [
        "# Warn-up Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t29Vf0t-adEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def sigmoid(x , derv = False):\n",
        "  if derv == False:\n",
        "    return 1 / np.sum(1, np.exp(-x))\n",
        "  return np.np.matmul(x,1-x)\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "x = np.random.rand(N, D_in)\n",
        "y = np.random.rand(N, D_out)\n",
        "# x(64,1000) y(64,10)\n",
        "\n",
        "w1 = np.random.randn(D_in, H)\n",
        "w2 = np.random.randn(H, D_out)\n",
        "# w1(1000, 100) w2(100,10)\n",
        "\n",
        "# (64,1000) (1000,100) (100,10) (10)\n",
        "\n",
        "lr = 0.000001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpfQNU8dchVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(500):\n",
        "  layer1 = x.dot(w1)\n",
        "  layer1_relu = np.maximum(layer1, 0 )\n",
        "  y_pred = layer1_relu.dot(w2)\n",
        "  \n",
        "  loss = np.sum(np.square(y_pred - y ))\n",
        "  print(\"epoch {:d} \\tloss {:.3f}\".format(epoch, loss))\n",
        "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "  grad_y_pred = 2.0 * (y_pred - y )\n",
        "  grad_w2 = layer1_relu.T.dot(grad_y_pred)\n",
        "  \n",
        "  grad_h_relu = grad_y_pred.dot(w2.T)\n",
        "  grad_h = grad_h_relu.copy()\n",
        "  grad_h[layer1 < 0] = 0\n",
        "  \n",
        "  grad_w1 = x.T.dot(grad_h)\n",
        "  \n",
        "  w1 -= lr * grad_w1\n",
        "  w2 -= lr * grad_w2\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUYGJ1aLc2BI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "x = torch.randn(N, D_in, device= device, dtype = dtype)\n",
        "y = torch.randn(N, D_out, device = device, dtype = dtype)\n",
        "\n",
        "w1 = torch.randn(D_in, H,device = device, dtype = dtype )\n",
        "w2 = torch.randn(H, D_out,device = device, dtype = dtype)\n",
        "\n",
        "lr = 0.000001\n",
        "\n",
        "for epoch in range(500):\n",
        "  h = x.mm(w1)\n",
        "  h_relu  = h.clamp(min = 0)\n",
        "  y_pred = h_relu.mm(w2)\n",
        "\n",
        "    # Compute and print loss\n",
        "  loss = (y_pred - y).pow(2).sum().item()\n",
        "  print(epoch, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "  grad_y_pred = 2.0 * (y_pred - y)\n",
        "  grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "  grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "  grad_h = grad_h_relu.clone()\n",
        "  grad_h[h < 0] = 0\n",
        "  grad_w1 = x.t().mm(grad_h)\n",
        "\n",
        "  w1 -= lr * grad_w1\n",
        "  w2 -= lr * grad_w2  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdIOi8EtqR4j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9200
        },
        "outputId": "eb25e561-b1a6-411f-e617-7169b3e599aa"
      },
      "source": [
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
        "for t in range(500):\n",
        "  y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "  loss = (y_pred - y).pow(2).sum()\n",
        "  print(t, loss.item())  \n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "    w1 -= lr * w1.grad\n",
        "    w2 -= lr * w2.grad\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "    w1.grad.zero_()\n",
        "    w2.grad.zero_()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 31707114.0\n",
            "1 28056354.0\n",
            "2 27665222.0\n",
            "3 26627110.0\n",
            "4 22571616.0\n",
            "5 16504395.0\n",
            "6 10437536.0\n",
            "7 6124219.0\n",
            "8 3549319.5\n",
            "9 2173410.0\n",
            "10 1444746.25\n",
            "11 1045667.25\n",
            "12 809199.375\n",
            "13 656182.375\n",
            "14 547944.625\n",
            "15 466164.09375\n",
            "16 401390.9375\n",
            "17 348535.15625\n",
            "18 304483.0\n",
            "19 267332.34375\n",
            "20 235667.1875\n",
            "21 208473.15625\n",
            "22 184991.296875\n",
            "23 164616.765625\n",
            "24 146887.21875\n",
            "25 131416.5\n",
            "26 117843.9453125\n",
            "27 105907.984375\n",
            "28 95365.515625\n",
            "29 86031.0859375\n",
            "30 77745.8671875\n",
            "31 70385.0\n",
            "32 63823.89453125\n",
            "33 57959.6015625\n",
            "34 52709.859375\n",
            "35 48000.5703125\n",
            "36 43775.0\n",
            "37 39971.51171875\n",
            "38 36542.25\n",
            "39 33444.54296875\n",
            "40 30642.556640625\n",
            "41 28107.3203125\n",
            "42 25811.154296875\n",
            "43 23727.685546875\n",
            "44 21833.55859375\n",
            "45 20109.58984375\n",
            "46 18540.208984375\n",
            "47 17107.970703125\n",
            "48 15799.2978515625\n",
            "49 14602.375\n",
            "50 13506.890625\n",
            "51 12502.5087890625\n",
            "52 11581.0361328125\n",
            "53 10734.9453125\n",
            "54 9957.44140625\n",
            "55 9242.048828125\n",
            "56 8583.46875\n",
            "57 7976.6201171875\n",
            "58 7417.087890625\n",
            "59 6900.802734375\n",
            "60 6424.1904296875\n",
            "61 5983.57421875\n",
            "62 5576.2236328125\n",
            "63 5199.33984375\n",
            "64 4850.4970703125\n",
            "65 4527.17431640625\n",
            "66 4227.43359375\n",
            "67 3949.426513671875\n",
            "68 3691.345458984375\n",
            "69 3451.741943359375\n",
            "70 3229.06689453125\n",
            "71 3022.001953125\n",
            "72 2829.356689453125\n",
            "73 2650.091552734375\n",
            "74 2483.12353515625\n",
            "75 2327.6025390625\n",
            "76 2182.7197265625\n",
            "77 2047.6123046875\n",
            "78 1921.54296875\n",
            "79 1803.858154296875\n",
            "80 1693.9576416015625\n",
            "81 1591.2904052734375\n",
            "82 1495.3133544921875\n",
            "83 1405.5662841796875\n",
            "84 1321.613525390625\n",
            "85 1243.0614013671875\n",
            "86 1169.515625\n",
            "87 1100.646484375\n",
            "88 1036.2576904296875\n",
            "89 975.9210815429688\n",
            "90 919.3392333984375\n",
            "91 866.266845703125\n",
            "92 816.47216796875\n",
            "93 769.7584228515625\n",
            "94 725.8919677734375\n",
            "95 684.6795043945312\n",
            "96 645.9761962890625\n",
            "97 609.599609375\n",
            "98 575.4061889648438\n",
            "99 543.263671875\n",
            "100 513.0379028320312\n",
            "101 484.5992431640625\n",
            "102 457.8350524902344\n",
            "103 432.6413879394531\n",
            "104 408.92071533203125\n",
            "105 386.5794372558594\n",
            "106 365.531982421875\n",
            "107 345.693603515625\n",
            "108 327.00164794921875\n",
            "109 309.37518310546875\n",
            "110 292.7513732910156\n",
            "111 277.0751953125\n",
            "112 262.2866516113281\n",
            "113 248.3311767578125\n",
            "114 235.1587371826172\n",
            "115 222.72824096679688\n",
            "116 210.98834228515625\n",
            "117 199.9003143310547\n",
            "118 189.42820739746094\n",
            "119 179.5322723388672\n",
            "120 170.1761474609375\n",
            "121 161.33480834960938\n",
            "122 152.97239685058594\n",
            "123 145.0680389404297\n",
            "124 137.5962677001953\n",
            "125 130.52490234375\n",
            "126 123.83138275146484\n",
            "127 117.49954223632812\n",
            "128 111.50810241699219\n",
            "129 105.83553314208984\n",
            "130 100.46672058105469\n",
            "131 95.38265991210938\n",
            "132 90.56766510009766\n",
            "133 86.0050048828125\n",
            "134 81.68073272705078\n",
            "135 77.58667755126953\n",
            "136 73.70503997802734\n",
            "137 70.02631378173828\n",
            "138 66.54045867919922\n",
            "139 63.234230041503906\n",
            "140 60.10041046142578\n",
            "141 57.12669372558594\n",
            "142 54.305824279785156\n",
            "143 51.6317138671875\n",
            "144 49.09324264526367\n",
            "145 46.68579864501953\n",
            "146 44.40088653564453\n",
            "147 42.23185348510742\n",
            "148 40.17247772216797\n",
            "149 38.21855545043945\n",
            "150 36.361602783203125\n",
            "151 34.59949493408203\n",
            "152 32.92519760131836\n",
            "153 31.335460662841797\n",
            "154 29.82546043395996\n",
            "155 28.390783309936523\n",
            "156 27.02733039855957\n",
            "157 25.73162078857422\n",
            "158 24.499832153320312\n",
            "159 23.32963752746582\n",
            "160 22.217893600463867\n",
            "161 21.159669876098633\n",
            "162 20.154329299926758\n",
            "163 19.198781967163086\n",
            "164 18.290006637573242\n",
            "165 17.424697875976562\n",
            "166 16.602649688720703\n",
            "167 15.819891929626465\n",
            "168 15.075653076171875\n",
            "169 14.367388725280762\n",
            "170 13.693429946899414\n",
            "171 13.052142143249512\n",
            "172 12.442049026489258\n",
            "173 11.861459732055664\n",
            "174 11.30848217010498\n",
            "175 10.782425880432129\n",
            "176 10.281129837036133\n",
            "177 9.803890228271484\n",
            "178 9.349960327148438\n",
            "179 8.917016983032227\n",
            "180 8.505302429199219\n",
            "181 8.112631797790527\n",
            "182 7.738863468170166\n",
            "183 7.382594108581543\n",
            "184 7.04366397857666\n",
            "185 6.720365047454834\n",
            "186 6.412392616271973\n",
            "187 6.119274139404297\n",
            "188 5.839441299438477\n",
            "189 5.572902679443359\n",
            "190 5.319028377532959\n",
            "191 5.076662063598633\n",
            "192 4.84605073928833\n",
            "193 4.6258134841918945\n",
            "194 4.416054725646973\n",
            "195 4.216127872467041\n",
            "196 4.025495529174805\n",
            "197 3.8434956073760986\n",
            "198 3.669808864593506\n",
            "199 3.5044100284576416\n",
            "200 3.3468172550201416\n",
            "201 3.196317195892334\n",
            "202 3.0527899265289307\n",
            "203 2.915815591812134\n",
            "204 2.7851479053497314\n",
            "205 2.6604764461517334\n",
            "206 2.5414862632751465\n",
            "207 2.428159475326538\n",
            "208 2.319643020629883\n",
            "209 2.216144323348999\n",
            "210 2.1176578998565674\n",
            "211 2.023408889770508\n",
            "212 1.9333910942077637\n",
            "213 1.8478634357452393\n",
            "214 1.7659116983413696\n",
            "215 1.6877185106277466\n",
            "216 1.6130563020706177\n",
            "217 1.541840672492981\n",
            "218 1.4737296104431152\n",
            "219 1.4086538553237915\n",
            "220 1.3465774059295654\n",
            "221 1.2873908281326294\n",
            "222 1.2308558225631714\n",
            "223 1.1767725944519043\n",
            "224 1.125067114830017\n",
            "225 1.0757498741149902\n",
            "226 1.028680443763733\n",
            "227 0.9835993647575378\n",
            "228 0.9406086802482605\n",
            "229 0.8995137810707092\n",
            "230 0.8602948784828186\n",
            "231 0.8228126764297485\n",
            "232 0.7868984341621399\n",
            "233 0.7527080178260803\n",
            "234 0.7199700474739075\n",
            "235 0.6887445449829102\n",
            "236 0.6588070392608643\n",
            "237 0.6302677989006042\n",
            "238 0.6028863787651062\n",
            "239 0.5768177509307861\n",
            "240 0.5518657565116882\n",
            "241 0.5280014872550964\n",
            "242 0.5051984190940857\n",
            "243 0.48339197039604187\n",
            "244 0.4625873565673828\n",
            "245 0.4426887035369873\n",
            "246 0.4236002564430237\n",
            "247 0.40536728501319885\n",
            "248 0.387973815202713\n",
            "249 0.3713264763355255\n",
            "250 0.35537099838256836\n",
            "251 0.34009233117103577\n",
            "252 0.3255738914012909\n",
            "253 0.3116597831249237\n",
            "254 0.2983176112174988\n",
            "255 0.2855057716369629\n",
            "256 0.2734150290489197\n",
            "257 0.26169565320014954\n",
            "258 0.2505105137825012\n",
            "259 0.23982203006744385\n",
            "260 0.22961780428886414\n",
            "261 0.2198060154914856\n",
            "262 0.21052348613739014\n",
            "263 0.20156489312648773\n",
            "264 0.19300396740436554\n",
            "265 0.1848383992910385\n",
            "266 0.17697301506996155\n",
            "267 0.16950859129428864\n",
            "268 0.1623108834028244\n",
            "269 0.15539869666099548\n",
            "270 0.14883103966712952\n",
            "271 0.1425386667251587\n",
            "272 0.13655950129032135\n",
            "273 0.13075779378414154\n",
            "274 0.12525978684425354\n",
            "275 0.11997479945421219\n",
            "276 0.11489372700452805\n",
            "277 0.11008754372596741\n",
            "278 0.10544228553771973\n",
            "279 0.10102012753486633\n",
            "280 0.096786729991436\n",
            "281 0.09269829094409943\n",
            "282 0.0888194888830185\n",
            "283 0.08509565889835358\n",
            "284 0.08153720945119858\n",
            "285 0.07814188301563263\n",
            "286 0.07486216723918915\n",
            "287 0.07172981649637222\n",
            "288 0.06873267143964767\n",
            "289 0.06588416546583176\n",
            "290 0.06312518566846848\n",
            "291 0.06049160286784172\n",
            "292 0.057966943830251694\n",
            "293 0.05554734170436859\n",
            "294 0.053237780928611755\n",
            "295 0.05101059004664421\n",
            "296 0.048901837319135666\n",
            "297 0.046883516013622284\n",
            "298 0.04492931440472603\n",
            "299 0.0430682972073555\n",
            "300 0.04126228764653206\n",
            "301 0.0395568311214447\n",
            "302 0.03789935261011124\n",
            "303 0.03635118156671524\n",
            "304 0.0348636768758297\n",
            "305 0.033425211906433105\n",
            "306 0.032038722187280655\n",
            "307 0.030726199969649315\n",
            "308 0.02946142666041851\n",
            "309 0.02825179137289524\n",
            "310 0.027082588523626328\n",
            "311 0.025975089520215988\n",
            "312 0.024909261614084244\n",
            "313 0.02387187071144581\n",
            "314 0.0228955689817667\n",
            "315 0.02196916937828064\n",
            "316 0.021070335060358047\n",
            "317 0.02021387405693531\n",
            "318 0.019388888031244278\n",
            "319 0.018590981140732765\n",
            "320 0.01783665269613266\n",
            "321 0.017106087878346443\n",
            "322 0.0164172500371933\n",
            "323 0.015750732272863388\n",
            "324 0.015123283490538597\n",
            "325 0.014515752904117107\n",
            "326 0.013925598934292793\n",
            "327 0.013360747136175632\n",
            "328 0.012826525606215\n",
            "329 0.012317787855863571\n",
            "330 0.011829041875898838\n",
            "331 0.011358669959008694\n",
            "332 0.010906769894063473\n",
            "333 0.01047565322369337\n",
            "334 0.010052322410047054\n",
            "335 0.009649541229009628\n",
            "336 0.009268789552152157\n",
            "337 0.008908781222999096\n",
            "338 0.008557267487049103\n",
            "339 0.008221239782869816\n",
            "340 0.007894104346632957\n",
            "341 0.007584380451589823\n",
            "342 0.007289078086614609\n",
            "343 0.007003717124462128\n",
            "344 0.006731943227350712\n",
            "345 0.006471648812294006\n",
            "346 0.006222249008715153\n",
            "347 0.005984141491353512\n",
            "348 0.005755748599767685\n",
            "349 0.005532919429242611\n",
            "350 0.005319749005138874\n",
            "351 0.005116509739309549\n",
            "352 0.004922620486468077\n",
            "353 0.004739654250442982\n",
            "354 0.004557171370834112\n",
            "355 0.004382048733532429\n",
            "356 0.004219829570502043\n",
            "357 0.00406311172991991\n",
            "358 0.003912318963557482\n",
            "359 0.003766121808439493\n",
            "360 0.003630466526374221\n",
            "361 0.0034932084381580353\n",
            "362 0.0033640104811638594\n",
            "363 0.0032403930090367794\n",
            "364 0.0031197399366647005\n",
            "365 0.0030097344424575567\n",
            "366 0.002901783911511302\n",
            "367 0.0027958867140114307\n",
            "368 0.002694417256861925\n",
            "369 0.0026019043289124966\n",
            "370 0.0025119753554463387\n",
            "371 0.0024247318506240845\n",
            "372 0.0023375002201646566\n",
            "373 0.0022567915730178356\n",
            "374 0.002177844289690256\n",
            "375 0.0021024122834205627\n",
            "376 0.0020303300116211176\n",
            "377 0.001959647051990032\n",
            "378 0.0018941361922770739\n",
            "379 0.0018282565288245678\n",
            "380 0.0017657597782090306\n",
            "381 0.0017087236046791077\n",
            "382 0.001650270656682551\n",
            "383 0.0015967185609042645\n",
            "384 0.0015466188779100776\n",
            "385 0.001495560398325324\n",
            "386 0.0014477632939815521\n",
            "387 0.0014014745829626918\n",
            "388 0.0013572475872933865\n",
            "389 0.001313599874265492\n",
            "390 0.0012735846685245633\n",
            "391 0.0012350291945040226\n",
            "392 0.0011958293616771698\n",
            "393 0.0011576483957469463\n",
            "394 0.0011236430145800114\n",
            "395 0.0010880609042942524\n",
            "396 0.0010541939409449697\n",
            "397 0.0010231233900412917\n",
            "398 0.000991252949461341\n",
            "399 0.0009612246649339795\n",
            "400 0.0009330037282779813\n",
            "401 0.000907127745449543\n",
            "402 0.000879600178450346\n",
            "403 0.0008535676752217114\n",
            "404 0.000829931115731597\n",
            "405 0.0008055439684540033\n",
            "406 0.0007828614325262606\n",
            "407 0.0007599436794407666\n",
            "408 0.0007392034749500453\n",
            "409 0.0007193107157945633\n",
            "410 0.0006986248190514743\n",
            "411 0.0006798174581490457\n",
            "412 0.0006612141150981188\n",
            "413 0.000642795639578253\n",
            "414 0.0006260507507249713\n",
            "415 0.0006088276859372854\n",
            "416 0.0005924443248659372\n",
            "417 0.0005773248849436641\n",
            "418 0.0005613226094283164\n",
            "419 0.0005481335683725774\n",
            "420 0.0005332366563379765\n",
            "421 0.0005192431854084134\n",
            "422 0.0005060262628830969\n",
            "423 0.0004941595252603292\n",
            "424 0.0004809582605957985\n",
            "425 0.0004692918446380645\n",
            "426 0.0004580675740726292\n",
            "427 0.0004467909748200327\n",
            "428 0.00043525081127882004\n",
            "429 0.00042443140409886837\n",
            "430 0.0004144974809605628\n",
            "431 0.00040475951391272247\n",
            "432 0.00039444639696739614\n",
            "433 0.0003857041010633111\n",
            "434 0.0003766012378036976\n",
            "435 0.00036753949825651944\n",
            "436 0.0003598041657824069\n",
            "437 0.0003501492028590292\n",
            "438 0.0003422897425480187\n",
            "439 0.00033429168979637325\n",
            "440 0.0003268525761086494\n",
            "441 0.00032030139118433\n",
            "442 0.0003129646647721529\n",
            "443 0.000305824214592576\n",
            "444 0.0002990756183862686\n",
            "445 0.0002929061884060502\n",
            "446 0.00028703149291686714\n",
            "447 0.0002802880189847201\n",
            "448 0.000274431164143607\n",
            "449 0.00026824421365745366\n",
            "450 0.00026337886811234057\n",
            "451 0.00025787693448364735\n",
            "452 0.0002525010204408318\n",
            "453 0.00024700970971025527\n",
            "454 0.00024234889133367687\n",
            "455 0.0002372960007051006\n",
            "456 0.00023318745661526918\n",
            "457 0.00022870175598654896\n",
            "458 0.00022407998039852828\n",
            "459 0.00021934366668574512\n",
            "460 0.00021534771076403558\n",
            "461 0.0002105658786604181\n",
            "462 0.00020631449297070503\n",
            "463 0.0002025248104473576\n",
            "464 0.00019869032257702202\n",
            "465 0.00019495910964906216\n",
            "466 0.00019135078764520586\n",
            "467 0.00018772369367070496\n",
            "468 0.00018419197294861078\n",
            "469 0.00018045563774649054\n",
            "470 0.00017683120677247643\n",
            "471 0.00017420490621589124\n",
            "472 0.00017073714116122574\n",
            "473 0.0001675827952567488\n",
            "474 0.0001646140735829249\n",
            "475 0.00016174263146240264\n",
            "476 0.00015859566337894648\n",
            "477 0.0001553539332235232\n",
            "478 0.00015304767293855548\n",
            "479 0.00015044474275782704\n",
            "480 0.00014809909043833613\n",
            "481 0.0001455297169741243\n",
            "482 0.00014284606731962413\n",
            "483 0.00014073392958380282\n",
            "484 0.00013779675646219403\n",
            "485 0.0001354924897896126\n",
            "486 0.00013345047773327678\n",
            "487 0.0001313762040808797\n",
            "488 0.00012895153486169875\n",
            "489 0.0001270307257073\n",
            "490 0.0001247788459295407\n",
            "491 0.00012293169857002795\n",
            "492 0.00012084029003744945\n",
            "493 0.00011920237739104778\n",
            "494 0.00011718863970600069\n",
            "495 0.00011515219375723973\n",
            "496 0.00011365259706508368\n",
            "497 0.00011184629693161696\n",
            "498 0.00011034370254492387\n",
            "499 0.00010863927309401333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0aSiL1AvDGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "class MyReLU(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, input):\n",
        "    ctx.save_for_backward(input)\n",
        "    return input.clamp(min=0)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}