{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/redman157/HocML/blob/master/pytorch_exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r9aUcVECieF",
        "colab_type": "code",
        "outputId": "7803d45c-1ed2-4c45-cdca-f199d2cc31fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "def imshow(img):\n",
        "  img = img / 2\n",
        "  npimg = img.numpy()\n",
        "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "  plt.show()\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 170041344/170498071 [00:15<00:00, 7991010.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "plane   cat   car  frog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3-2CQuRFdB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "net = Net()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnNBuECTGQMx",
        "colab_type": "code",
        "outputId": "2f86a7b0-07eb-49be-f0b8-6b3904ce854a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 2.217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r170500096it [00:30, 7991010.31it/s]                               "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,  4000] loss: 1.868\n",
            "[1,  6000] loss: 1.675\n",
            "[1,  8000] loss: 1.557\n",
            "[1, 10000] loss: 1.504\n",
            "[1, 12000] loss: 1.436\n",
            "[2,  2000] loss: 1.390\n",
            "[2,  4000] loss: 1.360\n",
            "[2,  6000] loss: 1.333\n",
            "[2,  8000] loss: 1.305\n",
            "[2, 10000] loss: 1.287\n",
            "[2, 12000] loss: 1.277\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjU4pNkfG9yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "\n",
        "outputs = net(images)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "imshow(torchvision.utils.make_grid(_))\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
        "                              for j in range(4)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0xvv1WbGjVW",
        "colab_type": "code",
        "outputId": "2f7d8a36-d5b3-41fe-efef-17c38811df26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 55 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOGHEZ9JRlVZ",
        "colab_type": "code",
        "outputId": "1d892290-243a-464e-d813-4f1ec0cf3882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "          x`\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of plane : 51 %\n",
            "Accuracy of   car : 70 %\n",
            "Accuracy of  bird : 34 %\n",
            "Accuracy of   cat : 48 %\n",
            "Accuracy of  deer : 37 %\n",
            "Accuracy of   dog : 40 %\n",
            "Accuracy of  frog : 71 %\n",
            "Accuracy of horse : 66 %\n",
            "Accuracy of  ship : 71 %\n",
            "Accuracy of truck : 66 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpaTqpIXaYo8",
        "colab_type": "text"
      },
      "source": [
        "# Warn-up Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t29Vf0t-adEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def sigmoid(x , derv = False):\n",
        "  if derv == False:\n",
        "    return 1 / np.sum(1, np.exp(-x))\n",
        "  return np.np.matmul(x,1-x)\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "x = np.random.rand(N, D_in)\n",
        "y = np.random.rand(N, D_out)\n",
        "# x(64,1000) y(64,10)\n",
        "\n",
        "w1 = np.random.randn(D_in, H)\n",
        "w2 = np.random.randn(H, D_out)\n",
        "# w1(1000, 100) w2(100,10)\n",
        "\n",
        "# (64,1000) (1000,100) (100,10) (10)\n",
        "\n",
        "lr = 0.000001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpfQNU8dchVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(500):\n",
        "  layer1 = x.dot(w1)\n",
        "  layer1_relu = np.maximum(layer1, 0 )\n",
        "  y_pred = layer1_relu.dot(w2)\n",
        "  \n",
        "  loss = np.sum(np.square(y_pred - y ))\n",
        "  print(\"epoch {:d} \\tloss {:.3f}\".format(epoch, loss))\n",
        "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "  grad_y_pred = 2.0 * (y_pred - y )\n",
        "  grad_w2 = layer1_relu.T.dot(grad_y_pred)\n",
        "  \n",
        "  grad_h_relu = grad_y_pred.dot(w2.T)\n",
        "  grad_h = grad_h_relu.copy()\n",
        "  grad_h[layer1 < 0] = 0\n",
        "  \n",
        "  grad_w1 = x.T.dot(grad_h)\n",
        "  \n",
        "  w1 -= lr * grad_w1\n",
        "  w2 -= lr * grad_w2\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUYGJ1aLc2BI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "x = torch.randn(N, D_in, device= device, dtype = dtype)\n",
        "y = torch.randn(N, D_out, device = device, dtype = dtype)\n",
        "\n",
        "w1 = torch.randn(D_in, H,device = device, dtype = dtype )\n",
        "w2 = torch.randn(H, D_out,device = device, dtype = dtype)\n",
        "\n",
        "lr = 0.000001\n",
        "\n",
        "for epoch in range(500):\n",
        "  h = x.mm(w1)\n",
        "  h_relu  = h.clamp(min = 0)\n",
        "  y_pred = h_relu.mm(w2)\n",
        "\n",
        "    # Compute and print loss\n",
        "  loss = (y_pred - y).pow(2).sum().item()\n",
        "  print(epoch, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "  grad_y_pred = 2.0 * (y_pred - y)\n",
        "  grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "  grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "  grad_h = grad_h_relu.clone()\n",
        "  grad_h[h < 0] = 0\n",
        "  grad_w1 = x.t().mm(grad_h)\n",
        "\n",
        "  w1 -= lr * grad_w1\n",
        "  w2 -= lr * grad_w2  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdIOi8EtqR4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
        "for t in range(500):\n",
        "  y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "  loss = (y_pred - y).pow(2).sum()\n",
        "  print(t, loss.item())  \n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "    w1 -= lr * w1.grad\n",
        "    w2 -= lr * w2.grad\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "    w1.grad.zero_()\n",
        "    w2.grad.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0aSiL1AvDGM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8517
        },
        "outputId": "eb67bad3-1c0b-45fc-e01c-21cc8db1d8f9"
      },
      "source": [
        "import torch\n",
        "from torch.nn import Linear, ReLU\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "model  = torch.nn.Sequential(\n",
        "    Linear(D_in, H),\n",
        "    ReLU(),\n",
        "    Linear(H, D_out),\n",
        ")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
        "for epoch in range(500):\n",
        "  y_pred = model(x)\n",
        "  loss = loss_fn(y_pred, y)\n",
        "  print(\"epoch {} \\t với mỗi loss {:.3f}\".format(epoch,loss))\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 \t với mỗi loss 673.933\n",
            "epoch 1 \t với mỗi loss 672.217\n",
            "epoch 2 \t với mỗi loss 670.504\n",
            "epoch 3 \t với mỗi loss 668.795\n",
            "epoch 4 \t với mỗi loss 667.090\n",
            "epoch 5 \t với mỗi loss 665.391\n",
            "epoch 6 \t với mỗi loss 663.698\n",
            "epoch 7 \t với mỗi loss 662.011\n",
            "epoch 8 \t với mỗi loss 660.327\n",
            "epoch 9 \t với mỗi loss 658.647\n",
            "epoch 10 \t với mỗi loss 656.970\n",
            "epoch 11 \t với mỗi loss 655.296\n",
            "epoch 12 \t với mỗi loss 653.625\n",
            "epoch 13 \t với mỗi loss 651.958\n",
            "epoch 14 \t với mỗi loss 650.294\n",
            "epoch 15 \t với mỗi loss 648.633\n",
            "epoch 16 \t với mỗi loss 646.977\n",
            "epoch 17 \t với mỗi loss 645.325\n",
            "epoch 18 \t với mỗi loss 643.679\n",
            "epoch 19 \t với mỗi loss 642.035\n",
            "epoch 20 \t với mỗi loss 640.397\n",
            "epoch 21 \t với mỗi loss 638.764\n",
            "epoch 22 \t với mỗi loss 637.136\n",
            "epoch 23 \t với mỗi loss 635.512\n",
            "epoch 24 \t với mỗi loss 633.891\n",
            "epoch 25 \t với mỗi loss 632.273\n",
            "epoch 26 \t với mỗi loss 630.661\n",
            "epoch 27 \t với mỗi loss 629.055\n",
            "epoch 28 \t với mỗi loss 627.451\n",
            "epoch 29 \t với mỗi loss 625.852\n",
            "epoch 30 \t với mỗi loss 624.257\n",
            "epoch 31 \t với mỗi loss 622.667\n",
            "epoch 32 \t với mỗi loss 621.083\n",
            "epoch 33 \t với mỗi loss 619.507\n",
            "epoch 34 \t với mỗi loss 617.936\n",
            "epoch 35 \t với mỗi loss 616.370\n",
            "epoch 36 \t với mỗi loss 614.809\n",
            "epoch 37 \t với mỗi loss 613.251\n",
            "epoch 38 \t với mỗi loss 611.698\n",
            "epoch 39 \t với mỗi loss 610.152\n",
            "epoch 40 \t với mỗi loss 608.615\n",
            "epoch 41 \t với mỗi loss 607.083\n",
            "epoch 42 \t với mỗi loss 605.554\n",
            "epoch 43 \t với mỗi loss 604.027\n",
            "epoch 44 \t với mỗi loss 602.505\n",
            "epoch 45 \t với mỗi loss 600.991\n",
            "epoch 46 \t với mỗi loss 599.481\n",
            "epoch 47 \t với mỗi loss 597.974\n",
            "epoch 48 \t với mỗi loss 596.473\n",
            "epoch 49 \t với mỗi loss 594.975\n",
            "epoch 50 \t với mỗi loss 593.479\n",
            "epoch 51 \t với mỗi loss 591.988\n",
            "epoch 52 \t với mỗi loss 590.502\n",
            "epoch 53 \t với mỗi loss 589.022\n",
            "epoch 54 \t với mỗi loss 587.547\n",
            "epoch 55 \t với mỗi loss 586.078\n",
            "epoch 56 \t với mỗi loss 584.613\n",
            "epoch 57 \t với mỗi loss 583.148\n",
            "epoch 58 \t với mỗi loss 581.687\n",
            "epoch 59 \t với mỗi loss 580.232\n",
            "epoch 60 \t với mỗi loss 578.781\n",
            "epoch 61 \t với mỗi loss 577.337\n",
            "epoch 62 \t với mỗi loss 575.896\n",
            "epoch 63 \t với mỗi loss 574.459\n",
            "epoch 64 \t với mỗi loss 573.026\n",
            "epoch 65 \t với mỗi loss 571.598\n",
            "epoch 66 \t với mỗi loss 570.175\n",
            "epoch 67 \t với mỗi loss 568.756\n",
            "epoch 68 \t với mỗi loss 567.341\n",
            "epoch 69 \t với mỗi loss 565.935\n",
            "epoch 70 \t với mỗi loss 564.536\n",
            "epoch 71 \t với mỗi loss 563.141\n",
            "epoch 72 \t với mỗi loss 561.750\n",
            "epoch 73 \t với mỗi loss 560.365\n",
            "epoch 74 \t với mỗi loss 558.983\n",
            "epoch 75 \t với mỗi loss 557.603\n",
            "epoch 76 \t với mỗi loss 556.225\n",
            "epoch 77 \t với mỗi loss 554.849\n",
            "epoch 78 \t với mỗi loss 553.477\n",
            "epoch 79 \t với mỗi loss 552.108\n",
            "epoch 80 \t với mỗi loss 550.743\n",
            "epoch 81 \t với mỗi loss 549.381\n",
            "epoch 82 \t với mỗi loss 548.025\n",
            "epoch 83 \t với mỗi loss 546.674\n",
            "epoch 84 \t với mỗi loss 545.328\n",
            "epoch 85 \t với mỗi loss 543.984\n",
            "epoch 86 \t với mỗi loss 542.644\n",
            "epoch 87 \t với mỗi loss 541.309\n",
            "epoch 88 \t với mỗi loss 539.978\n",
            "epoch 89 \t với mỗi loss 538.649\n",
            "epoch 90 \t với mỗi loss 537.324\n",
            "epoch 91 \t với mỗi loss 536.002\n",
            "epoch 92 \t với mỗi loss 534.681\n",
            "epoch 93 \t với mỗi loss 533.364\n",
            "epoch 94 \t với mỗi loss 532.051\n",
            "epoch 95 \t với mỗi loss 530.742\n",
            "epoch 96 \t với mỗi loss 529.438\n",
            "epoch 97 \t với mỗi loss 528.138\n",
            "epoch 98 \t với mỗi loss 526.839\n",
            "epoch 99 \t với mỗi loss 525.545\n",
            "epoch 100 \t với mỗi loss 524.255\n",
            "epoch 101 \t với mỗi loss 522.973\n",
            "epoch 102 \t với mỗi loss 521.695\n",
            "epoch 103 \t với mỗi loss 520.423\n",
            "epoch 104 \t với mỗi loss 519.151\n",
            "epoch 105 \t với mỗi loss 517.882\n",
            "epoch 106 \t với mỗi loss 516.618\n",
            "epoch 107 \t với mỗi loss 515.357\n",
            "epoch 108 \t với mỗi loss 514.102\n",
            "epoch 109 \t với mỗi loss 512.850\n",
            "epoch 110 \t với mỗi loss 511.602\n",
            "epoch 111 \t với mỗi loss 510.356\n",
            "epoch 112 \t với mỗi loss 509.114\n",
            "epoch 113 \t với mỗi loss 507.875\n",
            "epoch 114 \t với mỗi loss 506.642\n",
            "epoch 115 \t với mỗi loss 505.414\n",
            "epoch 116 \t với mỗi loss 504.184\n",
            "epoch 117 \t với mỗi loss 502.956\n",
            "epoch 118 \t với mỗi loss 501.734\n",
            "epoch 119 \t với mỗi loss 500.514\n",
            "epoch 120 \t với mỗi loss 499.298\n",
            "epoch 121 \t với mỗi loss 498.085\n",
            "epoch 122 \t với mỗi loss 496.875\n",
            "epoch 123 \t với mỗi loss 495.665\n",
            "epoch 124 \t với mỗi loss 494.459\n",
            "epoch 125 \t với mỗi loss 493.257\n",
            "epoch 126 \t với mỗi loss 492.057\n",
            "epoch 127 \t với mỗi loss 490.861\n",
            "epoch 128 \t với mỗi loss 489.670\n",
            "epoch 129 \t với mỗi loss 488.483\n",
            "epoch 130 \t với mỗi loss 487.299\n",
            "epoch 131 \t với mỗi loss 486.120\n",
            "epoch 132 \t với mỗi loss 484.944\n",
            "epoch 133 \t với mỗi loss 483.774\n",
            "epoch 134 \t với mỗi loss 482.608\n",
            "epoch 135 \t với mỗi loss 481.443\n",
            "epoch 136 \t với mỗi loss 480.281\n",
            "epoch 137 \t với mỗi loss 479.123\n",
            "epoch 138 \t với mỗi loss 477.968\n",
            "epoch 139 \t với mỗi loss 476.815\n",
            "epoch 140 \t với mỗi loss 475.665\n",
            "epoch 141 \t với mỗi loss 474.519\n",
            "epoch 142 \t với mỗi loss 473.376\n",
            "epoch 143 \t với mỗi loss 472.237\n",
            "epoch 144 \t với mỗi loss 471.099\n",
            "epoch 145 \t với mỗi loss 469.963\n",
            "epoch 146 \t với mỗi loss 468.830\n",
            "epoch 147 \t với mỗi loss 467.700\n",
            "epoch 148 \t với mỗi loss 466.574\n",
            "epoch 149 \t với mỗi loss 465.452\n",
            "epoch 150 \t với mỗi loss 464.336\n",
            "epoch 151 \t với mỗi loss 463.222\n",
            "epoch 152 \t với mỗi loss 462.109\n",
            "epoch 153 \t với mỗi loss 460.998\n",
            "epoch 154 \t với mỗi loss 459.891\n",
            "epoch 155 \t với mỗi loss 458.787\n",
            "epoch 156 \t với mỗi loss 457.685\n",
            "epoch 157 \t với mỗi loss 456.587\n",
            "epoch 158 \t với mỗi loss 455.490\n",
            "epoch 159 \t với mỗi loss 454.398\n",
            "epoch 160 \t với mỗi loss 453.311\n",
            "epoch 161 \t với mỗi loss 452.226\n",
            "epoch 162 \t với mỗi loss 451.144\n",
            "epoch 163 \t với mỗi loss 450.066\n",
            "epoch 164 \t với mỗi loss 448.991\n",
            "epoch 165 \t với mỗi loss 447.919\n",
            "epoch 166 \t với mỗi loss 446.852\n",
            "epoch 167 \t với mỗi loss 445.787\n",
            "epoch 168 \t với mỗi loss 444.726\n",
            "epoch 169 \t với mỗi loss 443.671\n",
            "epoch 170 \t với mỗi loss 442.615\n",
            "epoch 171 \t với mỗi loss 441.560\n",
            "epoch 172 \t với mỗi loss 440.510\n",
            "epoch 173 \t với mỗi loss 439.463\n",
            "epoch 174 \t với mỗi loss 438.419\n",
            "epoch 175 \t với mỗi loss 437.377\n",
            "epoch 176 \t với mỗi loss 436.338\n",
            "epoch 177 \t với mỗi loss 435.299\n",
            "epoch 178 \t với mỗi loss 434.264\n",
            "epoch 179 \t với mỗi loss 433.231\n",
            "epoch 180 \t với mỗi loss 432.202\n",
            "epoch 181 \t với mỗi loss 431.177\n",
            "epoch 182 \t với mỗi loss 430.154\n",
            "epoch 183 \t với mỗi loss 429.134\n",
            "epoch 184 \t với mỗi loss 428.117\n",
            "epoch 185 \t với mỗi loss 427.101\n",
            "epoch 186 \t với mỗi loss 426.088\n",
            "epoch 187 \t với mỗi loss 425.076\n",
            "epoch 188 \t với mỗi loss 424.068\n",
            "epoch 189 \t với mỗi loss 423.061\n",
            "epoch 190 \t với mỗi loss 422.059\n",
            "epoch 191 \t với mỗi loss 421.058\n",
            "epoch 192 \t với mỗi loss 420.060\n",
            "epoch 193 \t với mỗi loss 419.064\n",
            "epoch 194 \t với mỗi loss 418.073\n",
            "epoch 195 \t với mỗi loss 417.086\n",
            "epoch 196 \t với mỗi loss 416.100\n",
            "epoch 197 \t với mỗi loss 415.117\n",
            "epoch 198 \t với mỗi loss 414.133\n",
            "epoch 199 \t với mỗi loss 413.154\n",
            "epoch 200 \t với mỗi loss 412.176\n",
            "epoch 201 \t với mỗi loss 411.203\n",
            "epoch 202 \t với mỗi loss 410.233\n",
            "epoch 203 \t với mỗi loss 409.266\n",
            "epoch 204 \t với mỗi loss 408.298\n",
            "epoch 205 \t với mỗi loss 407.333\n",
            "epoch 206 \t với mỗi loss 406.373\n",
            "epoch 207 \t với mỗi loss 405.417\n",
            "epoch 208 \t với mỗi loss 404.464\n",
            "epoch 209 \t với mỗi loss 403.514\n",
            "epoch 210 \t với mỗi loss 402.567\n",
            "epoch 211 \t với mỗi loss 401.623\n",
            "epoch 212 \t với mỗi loss 400.682\n",
            "epoch 213 \t với mỗi loss 399.742\n",
            "epoch 214 \t với mỗi loss 398.804\n",
            "epoch 215 \t với mỗi loss 397.867\n",
            "epoch 216 \t với mỗi loss 396.930\n",
            "epoch 217 \t với mỗi loss 395.998\n",
            "epoch 218 \t với mỗi loss 395.068\n",
            "epoch 219 \t với mỗi loss 394.139\n",
            "epoch 220 \t với mỗi loss 393.214\n",
            "epoch 221 \t với mỗi loss 392.293\n",
            "epoch 222 \t với mỗi loss 391.375\n",
            "epoch 223 \t với mỗi loss 390.459\n",
            "epoch 224 \t với mỗi loss 389.545\n",
            "epoch 225 \t với mỗi loss 388.634\n",
            "epoch 226 \t với mỗi loss 387.726\n",
            "epoch 227 \t với mỗi loss 386.818\n",
            "epoch 228 \t với mỗi loss 385.914\n",
            "epoch 229 \t với mỗi loss 385.012\n",
            "epoch 230 \t với mỗi loss 384.112\n",
            "epoch 231 \t với mỗi loss 383.215\n",
            "epoch 232 \t với mỗi loss 382.323\n",
            "epoch 233 \t với mỗi loss 381.431\n",
            "epoch 234 \t với mỗi loss 380.543\n",
            "epoch 235 \t với mỗi loss 379.656\n",
            "epoch 236 \t với mỗi loss 378.771\n",
            "epoch 237 \t với mỗi loss 377.887\n",
            "epoch 238 \t với mỗi loss 377.005\n",
            "epoch 239 \t với mỗi loss 376.125\n",
            "epoch 240 \t với mỗi loss 375.249\n",
            "epoch 241 \t với mỗi loss 374.375\n",
            "epoch 242 \t với mỗi loss 373.504\n",
            "epoch 243 \t với mỗi loss 372.637\n",
            "epoch 244 \t với mỗi loss 371.778\n",
            "epoch 245 \t với mỗi loss 370.923\n",
            "epoch 246 \t với mỗi loss 370.068\n",
            "epoch 247 \t với mỗi loss 369.215\n",
            "epoch 248 \t với mỗi loss 368.361\n",
            "epoch 249 \t với mỗi loss 367.511\n",
            "epoch 250 \t với mỗi loss 366.663\n",
            "epoch 251 \t với mỗi loss 365.816\n",
            "epoch 252 \t với mỗi loss 364.971\n",
            "epoch 253 \t với mỗi loss 364.128\n",
            "epoch 254 \t với mỗi loss 363.288\n",
            "epoch 255 \t với mỗi loss 362.450\n",
            "epoch 256 \t với mỗi loss 361.613\n",
            "epoch 257 \t với mỗi loss 360.779\n",
            "epoch 258 \t với mỗi loss 359.947\n",
            "epoch 259 \t với mỗi loss 359.117\n",
            "epoch 260 \t với mỗi loss 358.290\n",
            "epoch 261 \t với mỗi loss 357.469\n",
            "epoch 262 \t với mỗi loss 356.650\n",
            "epoch 263 \t với mỗi loss 355.833\n",
            "epoch 264 \t với mỗi loss 355.016\n",
            "epoch 265 \t với mỗi loss 354.203\n",
            "epoch 266 \t với mỗi loss 353.393\n",
            "epoch 267 \t với mỗi loss 352.586\n",
            "epoch 268 \t với mỗi loss 351.779\n",
            "epoch 269 \t với mỗi loss 350.972\n",
            "epoch 270 \t với mỗi loss 350.168\n",
            "epoch 271 \t với mỗi loss 349.368\n",
            "epoch 272 \t với mỗi loss 348.570\n",
            "epoch 273 \t với mỗi loss 347.776\n",
            "epoch 274 \t với mỗi loss 346.984\n",
            "epoch 275 \t với mỗi loss 346.195\n",
            "epoch 276 \t với mỗi loss 345.407\n",
            "epoch 277 \t với mỗi loss 344.621\n",
            "epoch 278 \t với mỗi loss 343.838\n",
            "epoch 279 \t với mỗi loss 343.055\n",
            "epoch 280 \t với mỗi loss 342.274\n",
            "epoch 281 \t với mỗi loss 341.496\n",
            "epoch 282 \t với mỗi loss 340.720\n",
            "epoch 283 \t với mỗi loss 339.945\n",
            "epoch 284 \t với mỗi loss 339.170\n",
            "epoch 285 \t với mỗi loss 338.397\n",
            "epoch 286 \t với mỗi loss 337.627\n",
            "epoch 287 \t với mỗi loss 336.858\n",
            "epoch 288 \t với mỗi loss 336.092\n",
            "epoch 289 \t với mỗi loss 335.328\n",
            "epoch 290 \t với mỗi loss 334.566\n",
            "epoch 291 \t với mỗi loss 333.803\n",
            "epoch 292 \t với mỗi loss 333.043\n",
            "epoch 293 \t với mỗi loss 332.286\n",
            "epoch 294 \t với mỗi loss 331.531\n",
            "epoch 295 \t với mỗi loss 330.777\n",
            "epoch 296 \t với mỗi loss 330.025\n",
            "epoch 297 \t với mỗi loss 329.275\n",
            "epoch 298 \t với mỗi loss 328.528\n",
            "epoch 299 \t với mỗi loss 327.783\n",
            "epoch 300 \t với mỗi loss 327.041\n",
            "epoch 301 \t với mỗi loss 326.300\n",
            "epoch 302 \t với mỗi loss 325.562\n",
            "epoch 303 \t với mỗi loss 324.826\n",
            "epoch 304 \t với mỗi loss 324.090\n",
            "epoch 305 \t với mỗi loss 323.355\n",
            "epoch 306 \t với mỗi loss 322.622\n",
            "epoch 307 \t với mỗi loss 321.892\n",
            "epoch 308 \t với mỗi loss 321.167\n",
            "epoch 309 \t với mỗi loss 320.441\n",
            "epoch 310 \t với mỗi loss 319.716\n",
            "epoch 311 \t với mỗi loss 318.994\n",
            "epoch 312 \t với mỗi loss 318.272\n",
            "epoch 313 \t với mỗi loss 317.552\n",
            "epoch 314 \t với mỗi loss 316.832\n",
            "epoch 315 \t với mỗi loss 316.114\n",
            "epoch 316 \t với mỗi loss 315.399\n",
            "epoch 317 \t với mỗi loss 314.686\n",
            "epoch 318 \t với mỗi loss 313.973\n",
            "epoch 319 \t với mỗi loss 313.264\n",
            "epoch 320 \t với mỗi loss 312.556\n",
            "epoch 321 \t với mỗi loss 311.851\n",
            "epoch 322 \t với mỗi loss 311.146\n",
            "epoch 323 \t với mỗi loss 310.444\n",
            "epoch 324 \t với mỗi loss 309.745\n",
            "epoch 325 \t với mỗi loss 309.046\n",
            "epoch 326 \t với mỗi loss 308.349\n",
            "epoch 327 \t với mỗi loss 307.654\n",
            "epoch 328 \t với mỗi loss 306.961\n",
            "epoch 329 \t với mỗi loss 306.270\n",
            "epoch 330 \t với mỗi loss 305.580\n",
            "epoch 331 \t với mỗi loss 304.892\n",
            "epoch 332 \t với mỗi loss 304.205\n",
            "epoch 333 \t với mỗi loss 303.519\n",
            "epoch 334 \t với mỗi loss 302.834\n",
            "epoch 335 \t với mỗi loss 302.150\n",
            "epoch 336 \t với mỗi loss 301.466\n",
            "epoch 337 \t với mỗi loss 300.785\n",
            "epoch 338 \t với mỗi loss 300.108\n",
            "epoch 339 \t với mỗi loss 299.432\n",
            "epoch 340 \t với mỗi loss 298.758\n",
            "epoch 341 \t với mỗi loss 298.085\n",
            "epoch 342 \t với mỗi loss 297.411\n",
            "epoch 343 \t với mỗi loss 296.740\n",
            "epoch 344 \t với mỗi loss 296.072\n",
            "epoch 345 \t với mỗi loss 295.405\n",
            "epoch 346 \t với mỗi loss 294.738\n",
            "epoch 347 \t với mỗi loss 294.074\n",
            "epoch 348 \t với mỗi loss 293.411\n",
            "epoch 349 \t với mỗi loss 292.750\n",
            "epoch 350 \t với mỗi loss 292.091\n",
            "epoch 351 \t với mỗi loss 291.433\n",
            "epoch 352 \t với mỗi loss 290.776\n",
            "epoch 353 \t với mỗi loss 290.123\n",
            "epoch 354 \t với mỗi loss 289.471\n",
            "epoch 355 \t với mỗi loss 288.819\n",
            "epoch 356 \t với mỗi loss 288.168\n",
            "epoch 357 \t với mỗi loss 287.519\n",
            "epoch 358 \t với mỗi loss 286.873\n",
            "epoch 359 \t với mỗi loss 286.227\n",
            "epoch 360 \t với mỗi loss 285.582\n",
            "epoch 361 \t với mỗi loss 284.939\n",
            "epoch 362 \t với mỗi loss 284.297\n",
            "epoch 363 \t với mỗi loss 283.657\n",
            "epoch 364 \t với mỗi loss 283.018\n",
            "epoch 365 \t với mỗi loss 282.381\n",
            "epoch 366 \t với mỗi loss 281.744\n",
            "epoch 367 \t với mỗi loss 281.110\n",
            "epoch 368 \t với mỗi loss 280.479\n",
            "epoch 369 \t với mỗi loss 279.850\n",
            "epoch 370 \t với mỗi loss 279.220\n",
            "epoch 371 \t với mỗi loss 278.592\n",
            "epoch 372 \t với mỗi loss 277.966\n",
            "epoch 373 \t với mỗi loss 277.340\n",
            "epoch 374 \t với mỗi loss 276.717\n",
            "epoch 375 \t với mỗi loss 276.093\n",
            "epoch 376 \t với mỗi loss 275.472\n",
            "epoch 377 \t với mỗi loss 274.854\n",
            "epoch 378 \t với mỗi loss 274.237\n",
            "epoch 379 \t với mỗi loss 273.621\n",
            "epoch 380 \t với mỗi loss 273.006\n",
            "epoch 381 \t với mỗi loss 272.393\n",
            "epoch 382 \t với mỗi loss 271.782\n",
            "epoch 383 \t với mỗi loss 271.171\n",
            "epoch 384 \t với mỗi loss 270.560\n",
            "epoch 385 \t với mỗi loss 269.951\n",
            "epoch 386 \t với mỗi loss 269.343\n",
            "epoch 387 \t với mỗi loss 268.737\n",
            "epoch 388 \t với mỗi loss 268.130\n",
            "epoch 389 \t với mỗi loss 267.525\n",
            "epoch 390 \t với mỗi loss 266.921\n",
            "epoch 391 \t với mỗi loss 266.319\n",
            "epoch 392 \t với mỗi loss 265.719\n",
            "epoch 393 \t với mỗi loss 265.120\n",
            "epoch 394 \t với mỗi loss 264.522\n",
            "epoch 395 \t với mỗi loss 263.924\n",
            "epoch 396 \t với mỗi loss 263.328\n",
            "epoch 397 \t với mỗi loss 262.733\n",
            "epoch 398 \t với mỗi loss 262.139\n",
            "epoch 399 \t với mỗi loss 261.547\n",
            "epoch 400 \t với mỗi loss 260.955\n",
            "epoch 401 \t với mỗi loss 260.365\n",
            "epoch 402 \t với mỗi loss 259.775\n",
            "epoch 403 \t với mỗi loss 259.188\n",
            "epoch 404 \t với mỗi loss 258.603\n",
            "epoch 405 \t với mỗi loss 258.019\n",
            "epoch 406 \t với mỗi loss 257.435\n",
            "epoch 407 \t với mỗi loss 256.852\n",
            "epoch 408 \t với mỗi loss 256.271\n",
            "epoch 409 \t với mỗi loss 255.691\n",
            "epoch 410 \t với mỗi loss 255.112\n",
            "epoch 411 \t với mỗi loss 254.534\n",
            "epoch 412 \t với mỗi loss 253.956\n",
            "epoch 413 \t với mỗi loss 253.381\n",
            "epoch 414 \t với mỗi loss 252.806\n",
            "epoch 415 \t với mỗi loss 252.232\n",
            "epoch 416 \t với mỗi loss 251.659\n",
            "epoch 417 \t với mỗi loss 251.086\n",
            "epoch 418 \t với mỗi loss 250.515\n",
            "epoch 419 \t với mỗi loss 249.946\n",
            "epoch 420 \t với mỗi loss 249.378\n",
            "epoch 421 \t với mỗi loss 248.810\n",
            "epoch 422 \t với mỗi loss 248.243\n",
            "epoch 423 \t với mỗi loss 247.677\n",
            "epoch 424 \t với mỗi loss 247.111\n",
            "epoch 425 \t với mỗi loss 246.547\n",
            "epoch 426 \t với mỗi loss 245.982\n",
            "epoch 427 \t với mỗi loss 245.417\n",
            "epoch 428 \t với mỗi loss 244.854\n",
            "epoch 429 \t với mỗi loss 244.293\n",
            "epoch 430 \t với mỗi loss 243.733\n",
            "epoch 431 \t với mỗi loss 243.172\n",
            "epoch 432 \t với mỗi loss 242.614\n",
            "epoch 433 \t với mỗi loss 242.058\n",
            "epoch 434 \t với mỗi loss 241.502\n",
            "epoch 435 \t với mỗi loss 240.948\n",
            "epoch 436 \t với mỗi loss 240.394\n",
            "epoch 437 \t với mỗi loss 239.843\n",
            "epoch 438 \t với mỗi loss 239.292\n",
            "epoch 439 \t với mỗi loss 238.742\n",
            "epoch 440 \t với mỗi loss 238.193\n",
            "epoch 441 \t với mỗi loss 237.645\n",
            "epoch 442 \t với mỗi loss 237.097\n",
            "epoch 443 \t với mỗi loss 236.550\n",
            "epoch 444 \t với mỗi loss 236.007\n",
            "epoch 445 \t với mỗi loss 235.462\n",
            "epoch 446 \t với mỗi loss 234.919\n",
            "epoch 447 \t với mỗi loss 234.378\n",
            "epoch 448 \t với mỗi loss 233.838\n",
            "epoch 449 \t với mỗi loss 233.299\n",
            "epoch 450 \t với mỗi loss 232.761\n",
            "epoch 451 \t với mỗi loss 232.225\n",
            "epoch 452 \t với mỗi loss 231.690\n",
            "epoch 453 \t với mỗi loss 231.157\n",
            "epoch 454 \t với mỗi loss 230.626\n",
            "epoch 455 \t với mỗi loss 230.096\n",
            "epoch 456 \t với mỗi loss 229.567\n",
            "epoch 457 \t với mỗi loss 229.039\n",
            "epoch 458 \t với mỗi loss 228.512\n",
            "epoch 459 \t với mỗi loss 227.985\n",
            "epoch 460 \t với mỗi loss 227.459\n",
            "epoch 461 \t với mỗi loss 226.936\n",
            "epoch 462 \t với mỗi loss 226.412\n",
            "epoch 463 \t với mỗi loss 225.892\n",
            "epoch 464 \t với mỗi loss 225.372\n",
            "epoch 465 \t với mỗi loss 224.853\n",
            "epoch 466 \t với mỗi loss 224.334\n",
            "epoch 467 \t với mỗi loss 223.816\n",
            "epoch 468 \t với mỗi loss 223.300\n",
            "epoch 469 \t với mỗi loss 222.783\n",
            "epoch 470 \t với mỗi loss 222.268\n",
            "epoch 471 \t với mỗi loss 221.754\n",
            "epoch 472 \t với mỗi loss 221.242\n",
            "epoch 473 \t với mỗi loss 220.730\n",
            "epoch 474 \t với mỗi loss 220.221\n",
            "epoch 475 \t với mỗi loss 219.711\n",
            "epoch 476 \t với mỗi loss 219.203\n",
            "epoch 477 \t với mỗi loss 218.696\n",
            "epoch 478 \t với mỗi loss 218.190\n",
            "epoch 479 \t với mỗi loss 217.686\n",
            "epoch 480 \t với mỗi loss 217.181\n",
            "epoch 481 \t với mỗi loss 216.679\n",
            "epoch 482 \t với mỗi loss 216.177\n",
            "epoch 483 \t với mỗi loss 215.676\n",
            "epoch 484 \t với mỗi loss 215.177\n",
            "epoch 485 \t với mỗi loss 214.678\n",
            "epoch 486 \t với mỗi loss 214.180\n",
            "epoch 487 \t với mỗi loss 213.683\n",
            "epoch 488 \t với mỗi loss 213.187\n",
            "epoch 489 \t với mỗi loss 212.692\n",
            "epoch 490 \t với mỗi loss 212.199\n",
            "epoch 491 \t với mỗi loss 211.707\n",
            "epoch 492 \t với mỗi loss 211.216\n",
            "epoch 493 \t với mỗi loss 210.726\n",
            "epoch 494 \t với mỗi loss 210.236\n",
            "epoch 495 \t với mỗi loss 209.747\n",
            "epoch 496 \t với mỗi loss 209.259\n",
            "epoch 497 \t với mỗi loss 208.772\n",
            "epoch 498 \t với mỗi loss 208.286\n",
            "epoch 499 \t với mỗi loss 207.802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI0rclqaO04o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "2764eaaf-ddc9-44b7-bc05-ead231352b2b"
      },
      "source": [
        "import torch.nn as nn\n",
        "class TwoLayerNet(nn.Module):\n",
        "  def __init__(self, D_in, H, D_out):\n",
        "    super(TwoLayerNet, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(D_in, H)\n",
        "    self.linear2 = torch.nn.Linear(H, D_out)\n",
        "  def forward(self,x):\n",
        "    h_relu = self.linear1(x).clamp(min=0)\n",
        "    y_pred = self.linear2(h_relu)\n",
        "    return y_pred\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10 \n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "model = TwoLayerNet(D_in,H,D_out)\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(100):\n",
        "  y_pred = model(x)\n",
        "  loss = criterion(y_pred , y)\n",
        "  print(\"{0}, {1:.3f}\".format(epoch, loss.item()))\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "    "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0, 641.185\n",
            "1, 594.636\n",
            "2, 553.899\n",
            "3, 518.149\n",
            "4, 486.259\n",
            "5, 457.465\n",
            "6, 431.128\n",
            "7, 406.932\n",
            "8, 384.522\n",
            "9, 363.460\n",
            "10, 343.595\n",
            "11, 324.884\n",
            "12, 307.204\n",
            "13, 290.372\n",
            "14, 274.448\n",
            "15, 259.336\n",
            "16, 244.945\n",
            "17, 231.208\n",
            "18, 218.133\n",
            "19, 205.692\n",
            "20, 193.882\n",
            "21, 182.633\n",
            "22, 171.951\n",
            "23, 161.787\n",
            "24, 152.161\n",
            "25, 143.044\n",
            "26, 134.418\n",
            "27, 126.243\n",
            "28, 118.502\n",
            "29, 111.197\n",
            "30, 104.303\n",
            "31, 97.809\n",
            "32, 91.677\n",
            "33, 85.930\n",
            "34, 80.545\n",
            "35, 75.481\n",
            "36, 70.719\n",
            "37, 66.261\n",
            "38, 62.091\n",
            "39, 58.183\n",
            "40, 54.527\n",
            "41, 51.094\n",
            "42, 47.877\n",
            "43, 44.868\n",
            "44, 42.054\n",
            "45, 39.427\n",
            "46, 36.972\n",
            "47, 34.679\n",
            "48, 32.534\n",
            "49, 30.532\n",
            "50, 28.659\n",
            "51, 26.911\n",
            "52, 25.269\n",
            "53, 23.735\n",
            "54, 22.295\n",
            "55, 20.949\n",
            "56, 19.693\n",
            "57, 18.514\n",
            "58, 17.411\n",
            "59, 16.380\n",
            "60, 15.415\n",
            "61, 14.511\n",
            "62, 13.666\n",
            "63, 12.873\n",
            "64, 12.132\n",
            "65, 11.438\n",
            "66, 10.788\n",
            "67, 10.177\n",
            "68, 9.604\n",
            "69, 9.066\n",
            "70, 8.556\n",
            "71, 8.079\n",
            "72, 7.629\n",
            "73, 7.207\n",
            "74, 6.811\n",
            "75, 6.438\n",
            "76, 6.088\n",
            "77, 5.759\n",
            "78, 5.449\n",
            "79, 5.158\n",
            "80, 4.883\n",
            "81, 4.625\n",
            "82, 4.381\n",
            "83, 4.152\n",
            "84, 3.935\n",
            "85, 3.730\n",
            "86, 3.538\n",
            "87, 3.356\n",
            "88, 3.185\n",
            "89, 3.023\n",
            "90, 2.871\n",
            "91, 2.727\n",
            "92, 2.590\n",
            "93, 2.461\n",
            "94, 2.339\n",
            "95, 2.224\n",
            "96, 2.115\n",
            "97, 2.012\n",
            "98, 1.914\n",
            "99, 1.821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfg8rWLpotKm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8517
        },
        "outputId": "0188a522-d908-4cee-c6b7-cde7b1a94e3e"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "class DynamicNet(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        \"\"\"\n",
        "        In the constructor we construct three nn.Linear instances that we will use\n",
        "        in the forward pass.\n",
        "        \"\"\"\n",
        "        super(DynamicNet, self).__init__()\n",
        "        self.input_linear = torch.nn.Linear(D_in, H)\n",
        "        self.middle_linear = torch.nn.Linear(H, H)\n",
        "        self.output_linear = torch.nn.Linear(H, D_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
        "        and reuse the middle_linear Module that many times to compute hidden layer\n",
        "        representations.\n",
        "\n",
        "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
        "        Python control-flow operators like loops or conditional statements when\n",
        "        defining the forward pass of the model.\n",
        "\n",
        "        Here we also see that it is perfectly safe to reuse the same Module many\n",
        "        times when defining a computational graph. This is a big improvement from Lua\n",
        "        Torch, where each Module could be used only once.\n",
        "        \"\"\"\n",
        "        h_relu = self.input_linear(x).clamp(min=0)\n",
        "        for _ in range(random.randint(0, 3)):\n",
        "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
        "        y_pred = self.output_linear(h_relu)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold inputs and outputs\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "# Construct our model by instantiating the class defined above\n",
        "model = DynamicNet(D_in, H, D_out)\n",
        "\n",
        "# Construct our loss function and an Optimizer. Training this strange model with\n",
        "# vanilla stochastic gradient descent is tough, so we use momentum\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
        "for t in range(500):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y)\n",
        "    print(t, loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 640.7794799804688\n",
            "1 668.235595703125\n",
            "2 616.2342529296875\n",
            "3 633.5201416015625\n",
            "4 629.8252563476562\n",
            "5 424.67596435546875\n",
            "6 621.7532958984375\n",
            "7 628.90966796875\n",
            "8 619.1146850585938\n",
            "9 610.5538330078125\n",
            "10 590.0617065429688\n",
            "11 248.057373046875\n",
            "12 554.19775390625\n",
            "13 531.3073120117188\n",
            "14 192.2250213623047\n",
            "15 476.1194152832031\n",
            "16 571.8756713867188\n",
            "17 558.6575317382812\n",
            "18 383.5961608886719\n",
            "19 585.29833984375\n",
            "20 320.4255065917969\n",
            "21 129.3243865966797\n",
            "22 124.92008209228516\n",
            "23 109.02921295166016\n",
            "24 503.34124755859375\n",
            "25 481.1589050292969\n",
            "26 68.94428253173828\n",
            "27 425.8083801269531\n",
            "28 195.23973083496094\n",
            "29 181.0964813232422\n",
            "30 47.32267379760742\n",
            "31 353.74530029296875\n",
            "32 44.217281341552734\n",
            "33 39.0440788269043\n",
            "34 31.79064178466797\n",
            "35 25.666040420532227\n",
            "36 337.4888610839844\n",
            "37 239.45281982421875\n",
            "38 134.14825439453125\n",
            "39 270.03680419921875\n",
            "40 30.7018985748291\n",
            "41 34.0453987121582\n",
            "42 117.27193450927734\n",
            "43 209.7183074951172\n",
            "44 30.602144241333008\n",
            "45 212.44244384765625\n",
            "46 202.0918731689453\n",
            "47 175.6474151611328\n",
            "48 120.9085693359375\n",
            "49 149.18492126464844\n",
            "50 122.93012237548828\n",
            "51 124.49093627929688\n",
            "52 58.64228057861328\n",
            "53 90.19981384277344\n",
            "54 87.36781311035156\n",
            "55 36.821861267089844\n",
            "56 30.564931869506836\n",
            "57 22.589160919189453\n",
            "58 16.630924224853516\n",
            "59 67.42371368408203\n",
            "60 110.9810562133789\n",
            "61 71.96733093261719\n",
            "62 146.6378631591797\n",
            "63 106.35067749023438\n",
            "64 74.34622192382812\n",
            "65 82.00772094726562\n",
            "66 83.46515655517578\n",
            "67 112.97406005859375\n",
            "68 35.1473274230957\n",
            "69 110.25416564941406\n",
            "70 108.70987701416016\n",
            "71 99.33747863769531\n",
            "72 66.30155181884766\n",
            "73 64.35798645019531\n",
            "74 37.19869613647461\n",
            "75 43.13632583618164\n",
            "76 61.26215744018555\n",
            "77 99.14202880859375\n",
            "78 121.94117736816406\n",
            "79 26.851762771606445\n",
            "80 83.98727416992188\n",
            "81 85.37853240966797\n",
            "82 117.78844451904297\n",
            "83 17.323646545410156\n",
            "84 21.9691162109375\n",
            "85 61.12104415893555\n",
            "86 45.2802619934082\n",
            "87 34.92911911010742\n",
            "88 80.14800262451172\n",
            "89 20.59501075744629\n",
            "90 57.47693634033203\n",
            "91 27.837820053100586\n",
            "92 35.77988052368164\n",
            "93 27.58327865600586\n",
            "94 27.38726234436035\n",
            "95 9.06136703491211\n",
            "96 26.487892150878906\n",
            "97 26.093034744262695\n",
            "98 19.159025192260742\n",
            "99 12.867362022399902\n",
            "100 13.54682731628418\n",
            "101 30.030550003051758\n",
            "102 45.26242446899414\n",
            "103 9.114575386047363\n",
            "104 38.53764343261719\n",
            "105 36.761131286621094\n",
            "106 12.72385025024414\n",
            "107 30.6920108795166\n",
            "108 6.836511135101318\n",
            "109 8.288422584533691\n",
            "110 37.322330474853516\n",
            "111 14.596436500549316\n",
            "112 12.367527961730957\n",
            "113 16.257884979248047\n",
            "114 16.658065795898438\n",
            "115 11.657217979431152\n",
            "116 6.329166889190674\n",
            "117 4.908199787139893\n",
            "118 8.626513481140137\n",
            "119 7.872135639190674\n",
            "120 23.770559310913086\n",
            "121 20.803688049316406\n",
            "122 8.995515823364258\n",
            "123 24.146259307861328\n",
            "124 5.371500492095947\n",
            "125 6.157474040985107\n",
            "126 5.740236759185791\n",
            "127 49.9780387878418\n",
            "128 6.6993632316589355\n",
            "129 12.406702041625977\n",
            "130 74.05982971191406\n",
            "131 6.312932014465332\n",
            "132 41.14708709716797\n",
            "133 10.86921215057373\n",
            "134 6.857386589050293\n",
            "135 34.33479309082031\n",
            "136 11.689846992492676\n",
            "137 5.90433931350708\n",
            "138 15.632455825805664\n",
            "139 29.76246452331543\n",
            "140 12.407480239868164\n",
            "141 3.7717511653900146\n",
            "142 28.63886070251465\n",
            "143 3.8728389739990234\n",
            "144 24.261520385742188\n",
            "145 4.955533981323242\n",
            "146 5.375290393829346\n",
            "147 8.267852783203125\n",
            "148 10.715673446655273\n",
            "149 12.938030242919922\n",
            "150 11.431069374084473\n",
            "151 4.5976643562316895\n",
            "152 5.049772262573242\n",
            "153 28.578603744506836\n",
            "154 16.191390991210938\n",
            "155 2.60711669921875\n",
            "156 4.9146952629089355\n",
            "157 4.441260814666748\n",
            "158 3.689330577850342\n",
            "159 34.31523895263672\n",
            "160 1.99717378616333\n",
            "161 8.459373474121094\n",
            "162 11.380714416503906\n",
            "163 9.986193656921387\n",
            "164 20.567522048950195\n",
            "165 25.98175048828125\n",
            "166 10.71967887878418\n",
            "167 3.605466365814209\n",
            "168 2.961361885070801\n",
            "169 27.152271270751953\n",
            "170 7.890091896057129\n",
            "171 10.7894926071167\n",
            "172 2.9900012016296387\n",
            "173 7.061142921447754\n",
            "174 9.125065803527832\n",
            "175 7.183894634246826\n",
            "176 2.625119209289551\n",
            "177 13.869134902954102\n",
            "178 12.589415550231934\n",
            "179 2.81860613822937\n",
            "180 1.947851538658142\n",
            "181 4.212658405303955\n",
            "182 3.8896210193634033\n",
            "183 2.53442645072937\n",
            "184 2.8021018505096436\n",
            "185 5.952911376953125\n",
            "186 2.450141668319702\n",
            "187 2.337998151779175\n",
            "188 4.985354900360107\n",
            "189 1.8854243755340576\n",
            "190 2.032355546951294\n",
            "191 2.387559175491333\n",
            "192 3.449065923690796\n",
            "193 2.806508779525757\n",
            "194 1.7116413116455078\n",
            "195 1.221601128578186\n",
            "196 3.2044835090637207\n",
            "197 1.9578568935394287\n",
            "198 2.7614328861236572\n",
            "199 1.2503081560134888\n",
            "200 1.0423957109451294\n",
            "201 3.4660725593566895\n",
            "202 2.5953712463378906\n",
            "203 1.684979796409607\n",
            "204 2.459277868270874\n",
            "205 2.139590263366699\n",
            "206 1.8151333332061768\n",
            "207 3.148808479309082\n",
            "208 1.2711119651794434\n",
            "209 1.7250964641571045\n",
            "210 2.155025005340576\n",
            "211 1.8779748678207397\n",
            "212 1.1713857650756836\n",
            "213 1.8034921884536743\n",
            "214 0.8392805457115173\n",
            "215 0.8110648393630981\n",
            "216 3.9376423358917236\n",
            "217 1.175073266029358\n",
            "218 0.6286696791648865\n",
            "219 0.7141984105110168\n",
            "220 1.617121934890747\n",
            "221 4.93418025970459\n",
            "222 0.8600389957427979\n",
            "223 1.4649754762649536\n",
            "224 1.8865833282470703\n",
            "225 2.0230023860931396\n",
            "226 1.7883868217468262\n",
            "227 2.452328681945801\n",
            "228 1.7188708782196045\n",
            "229 2.677619695663452\n",
            "230 1.5604873895645142\n",
            "231 1.302811861038208\n",
            "232 1.0782842636108398\n",
            "233 2.0898330211639404\n",
            "234 0.6684210896492004\n",
            "235 0.5609081387519836\n",
            "236 1.568394422531128\n",
            "237 0.43901315331459045\n",
            "238 2.8738820552825928\n",
            "239 0.3055863082408905\n",
            "240 1.7045984268188477\n",
            "241 0.8024339079856873\n",
            "242 2.698594808578491\n",
            "243 1.7127571105957031\n",
            "244 1.5506094694137573\n",
            "245 0.8333656787872314\n",
            "246 0.918870747089386\n",
            "247 1.1145814657211304\n",
            "248 2.1374356746673584\n",
            "249 0.6473101377487183\n",
            "250 1.1487964391708374\n",
            "251 1.4059504270553589\n",
            "252 1.2411712408065796\n",
            "253 0.5765037536621094\n",
            "254 0.5865880250930786\n",
            "255 1.5753639936447144\n",
            "256 0.6467108726501465\n",
            "257 0.7575188875198364\n",
            "258 1.4813412427902222\n",
            "259 0.9301355481147766\n",
            "260 0.5552311539649963\n",
            "261 0.8316233158111572\n",
            "262 0.4336140751838684\n",
            "263 1.0959571599960327\n",
            "264 0.4055023491382599\n",
            "265 0.3963601887226105\n",
            "266 1.5984493494033813\n",
            "267 1.1012647151947021\n",
            "268 0.9067690372467041\n",
            "269 0.16755421459674835\n",
            "270 0.2383025735616684\n",
            "271 2.0102455615997314\n",
            "272 2.0920352935791016\n",
            "273 0.8207074403762817\n",
            "274 0.41451296210289\n",
            "275 3.615051507949829\n",
            "276 1.6687129735946655\n",
            "277 0.1463755965232849\n",
            "278 0.8560009598731995\n",
            "279 0.8178957104682922\n",
            "280 1.4298288822174072\n",
            "281 0.6586825847625732\n",
            "282 1.0042520761489868\n",
            "283 3.5980629920959473\n",
            "284 0.6471057534217834\n",
            "285 0.4590861201286316\n",
            "286 0.7143133282661438\n",
            "287 1.8066097497940063\n",
            "288 0.3801928460597992\n",
            "289 0.630791425704956\n",
            "290 1.5380594730377197\n",
            "291 0.25835105776786804\n",
            "292 1.7390857934951782\n",
            "293 1.0232079029083252\n",
            "294 0.9219776391983032\n",
            "295 0.9703973531723022\n",
            "296 0.6899539828300476\n",
            "297 0.8982543349266052\n",
            "298 1.1756783723831177\n",
            "299 0.4616641402244568\n",
            "300 0.8478077054023743\n",
            "301 0.7652706503868103\n",
            "302 0.7393531203269958\n",
            "303 0.9992828965187073\n",
            "304 0.6935967803001404\n",
            "305 0.6014840602874756\n",
            "306 1.0568017959594727\n",
            "307 1.4807994365692139\n",
            "308 0.7078127264976501\n",
            "309 1.3528211116790771\n",
            "310 0.9658717513084412\n",
            "311 0.313472181558609\n",
            "312 1.5499144792556763\n",
            "313 0.7143397331237793\n",
            "314 0.2513139545917511\n",
            "315 0.9080417156219482\n",
            "316 0.22909536957740784\n",
            "317 0.7626363039016724\n",
            "318 0.12870502471923828\n",
            "319 0.8104747533798218\n",
            "320 0.515171468257904\n",
            "321 0.40325799584388733\n",
            "322 0.866387665271759\n",
            "323 0.75669926404953\n",
            "324 0.7711841464042664\n",
            "325 0.6295340657234192\n",
            "326 0.47778522968292236\n",
            "327 0.5799192190170288\n",
            "328 0.35127460956573486\n",
            "329 0.8868144750595093\n",
            "330 0.43263304233551025\n",
            "331 0.7937544584274292\n",
            "332 0.9878205060958862\n",
            "333 0.14586320519447327\n",
            "334 0.13017894327640533\n",
            "335 1.038731575012207\n",
            "336 0.11369460821151733\n",
            "337 0.6405918598175049\n",
            "338 0.29037514328956604\n",
            "339 0.8631452322006226\n",
            "340 0.9063764214515686\n",
            "341 1.09908926486969\n",
            "342 0.49842435121536255\n",
            "343 0.6222826242446899\n",
            "344 0.316997766494751\n",
            "345 1.2475311756134033\n",
            "346 0.568248987197876\n",
            "347 0.5801194310188293\n",
            "348 0.6157429218292236\n",
            "349 0.7987170815467834\n",
            "350 0.5491798520088196\n",
            "351 0.8226966261863708\n",
            "352 0.8465189933776855\n",
            "353 0.13621768355369568\n",
            "354 0.4743961989879608\n",
            "355 0.3641963303089142\n",
            "356 0.31572145223617554\n",
            "357 0.46000736951828003\n",
            "358 0.40163639187812805\n",
            "359 0.2773039937019348\n",
            "360 0.8653576970100403\n",
            "361 1.8368399143218994\n",
            "362 0.6875491738319397\n",
            "363 0.7881391048431396\n",
            "364 0.8073892593383789\n",
            "365 1.9411109685897827\n",
            "366 0.5008035898208618\n",
            "367 1.8783005475997925\n",
            "368 1.0228166580200195\n",
            "369 0.7998992800712585\n",
            "370 0.5949034094810486\n",
            "371 0.8356248736381531\n",
            "372 0.9780819416046143\n",
            "373 0.8020406365394592\n",
            "374 2.670027256011963\n",
            "375 2.105395555496216\n",
            "376 2.6629092693328857\n",
            "377 5.930727481842041\n",
            "378 4.130067348480225\n",
            "379 0.6495577692985535\n",
            "380 0.6001302599906921\n",
            "381 3.9298434257507324\n",
            "382 9.523983001708984\n",
            "383 0.7837859392166138\n",
            "384 3.161616086959839\n",
            "385 0.8239395022392273\n",
            "386 39.11669158935547\n",
            "387 0.7405663728713989\n",
            "388 9.410762786865234\n",
            "389 37.91654586791992\n",
            "390 9.505773544311523\n",
            "391 7.346708297729492\n",
            "392 3.0258076190948486\n",
            "393 11.245837211608887\n",
            "394 15.130102157592773\n",
            "395 10.894474029541016\n",
            "396 3.068894386291504\n",
            "397 0.670011043548584\n",
            "398 1.1933281421661377\n",
            "399 32.81680679321289\n",
            "400 1.7706648111343384\n",
            "401 0.762420654296875\n",
            "402 21.35098648071289\n",
            "403 4.4489665031433105\n",
            "404 5.501991271972656\n",
            "405 19.449756622314453\n",
            "406 3.824265480041504\n",
            "407 1.194008708000183\n",
            "408 6.423901081085205\n",
            "409 2.7298357486724854\n",
            "410 9.572213172912598\n",
            "411 3.963380813598633\n",
            "412 20.382963180541992\n",
            "413 7.0440592765808105\n",
            "414 12.524778366088867\n",
            "415 14.471104621887207\n",
            "416 1.4243801832199097\n",
            "417 1.1092301607131958\n",
            "418 0.7966150641441345\n",
            "419 9.956835746765137\n",
            "420 3.230804681777954\n",
            "421 1.3103080987930298\n",
            "422 13.535258293151855\n",
            "423 2.648897409439087\n",
            "424 1.9855244159698486\n",
            "425 2.4066238403320312\n",
            "426 2.809814453125\n",
            "427 2.118196964263916\n",
            "428 2.2896246910095215\n",
            "429 1.115228533744812\n",
            "430 13.140624046325684\n",
            "431 0.7762812376022339\n",
            "432 1.2470011711120605\n",
            "433 1.9028167724609375\n",
            "434 3.2691421508789062\n",
            "435 0.896763265132904\n",
            "436 1.1466509103775024\n",
            "437 1.3509013652801514\n",
            "438 2.614699602127075\n",
            "439 0.7835668921470642\n",
            "440 2.3162131309509277\n",
            "441 0.8021333813667297\n",
            "442 1.64530611038208\n",
            "443 0.7695294618606567\n",
            "444 1.8685545921325684\n",
            "445 0.6248503923416138\n",
            "446 0.746965765953064\n",
            "447 1.3717228174209595\n",
            "448 0.5734965205192566\n",
            "449 0.704282820224762\n",
            "450 0.7471035122871399\n",
            "451 1.3137348890304565\n",
            "452 0.5539524555206299\n",
            "453 0.6320514678955078\n",
            "454 0.9529454708099365\n",
            "455 1.226943850517273\n",
            "456 0.43902772665023804\n",
            "457 7.984224796295166\n",
            "458 1.5182538032531738\n",
            "459 1.2039861679077148\n",
            "460 3.4167189598083496\n",
            "461 1.4722704887390137\n",
            "462 0.7351686954498291\n",
            "463 1.03043532371521\n",
            "464 2.137683868408203\n",
            "465 0.817882776260376\n",
            "466 2.1112477779388428\n",
            "467 0.331955224275589\n",
            "468 1.2578094005584717\n",
            "469 2.0687813758850098\n",
            "470 1.653443694114685\n",
            "471 0.5381482243537903\n",
            "472 1.0684462785720825\n",
            "473 0.3374820351600647\n",
            "474 0.4051867425441742\n",
            "475 0.8166913390159607\n",
            "476 0.6282320022583008\n",
            "477 0.4618603587150574\n",
            "478 0.23645982146263123\n",
            "479 1.4876866340637207\n",
            "480 0.3724302351474762\n",
            "481 0.3148767054080963\n",
            "482 0.6423184275627136\n",
            "483 0.4904020130634308\n",
            "484 0.38526567816734314\n",
            "485 0.5922693610191345\n",
            "486 0.2587256133556366\n",
            "487 0.2729666531085968\n",
            "488 0.3374900817871094\n",
            "489 0.2950616776943207\n",
            "490 0.23571686446666718\n",
            "491 0.2224322259426117\n",
            "492 0.23276488482952118\n",
            "493 0.6089041233062744\n",
            "494 0.19791151583194733\n",
            "495 0.1779034435749054\n",
            "496 0.6443143486976624\n",
            "497 0.16226676106452942\n",
            "498 0.14463376998901367\n",
            "499 0.5142707228660583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4y5ZS28jusp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}